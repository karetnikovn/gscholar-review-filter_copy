{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:51:46.732751400Z",
     "start_time": "2023-10-12T10:51:37.171459200Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "!pip3 install -U scholarly\n",
    "!sudo apt-get install tor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:53:16.215026Z",
     "start_time": "2023-10-12T10:53:13.958529300Z"
    }
   },
   "id": "33c67ba2d2e9f3ff"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'Can Generative AI Improve Social Science Research?[1]\\n\\nChris Bail\\nDuke University\\nwww.chrisbail.net\\n\\nAbstract. Artificial intelligence that can produce realistic text, images, and\\nother human-like outputs is currently transforming many different industries.\\nYet it is not yet known how such tools might transform social science research. In\\nthe first section of this article, I assess the potential of Generative AI to improve\\nonline experiments, agent-based models, and automated content analyses. I also\\ndiscuss whether these tools may help social scientists perform literature reviews,\\nidentify novel research questions, and develop hypotheses to explain them. Next,\\nI evaluate whether Generative AI can help social scientists with more mundane\\ntasks such as acquiring advanced programming skills or writing more effective\\nprose. In the second section of this article I discuss the limitations of Generative\\nAI as well as how these tools might be employed by researchers in an ethical\\nmanner. I discuss how bias in the processes and data used to train these tools can\\nnegatively impact social science research as well as a range of other challenges\\nrelated to accuracy, reproducibility, interpretability, and efficiency. I conclude by\\nhighlighting the need for increased collaboration between social scientists and\\nartificial intelligence researchers— not only to ensure that such tools are used in\\na safe and ethical manner, but also because the progress of artificial intelligence\\nmay require deeper understanding of theories of human behavior.\\n\\nIntroduction\\n\\nGenerative Artificial Intelligence— technology capable of producing realistic text,\\nimages, music, and other creative forms— continues to captivate large audiences.\\nChatGPT, the conversational chatbot generated by OpenAI, recently became the\\nfastest-growing consumer application in history. According to one report, this tool\\namassed more than 13 million unique users each day in January 2023.[2] There is\\nwidespread speculation that such generative AI will have considerable impact on\\na range of different industries— from creative and legal writing to advertising\\nand customer service. Yet sociologists, political scientists, economists and other\\nsocial scientists are only beginning to explore how generative AI will transform\\ntheir research. In this article, I evaluate whether social scientists can employ\\nGenerative AI to enhance conventional research methods, but also invent entirely\\nnew forms of inquiry as well. Along the way, I evaluate whether the use of such\\ntools is ethical in research settings, and how scholars interested in exploring\\nsuch technologies might mitigate the various risks associated with these largely\\nuntested technologies.\\n\\nIn the first section of this article, I ask whether Generative AI can effectively\\n\\n1\\n\\n\\x0csimulate human behavior for the purposes of social science research. I examine\\nwhether these tools may be useful for creating synthetic human-like content\\nsuch as images used in survey experiments or text for vignette studies. Next,\\nI review recent studies that employ Generative AI models to simulate human\\npopulations taking public opinion surveys, impersonate team-members in online\\nexperiments, and provide more realistic agent-based models. I then ask whether\\nGenerative AI can become a “virtual research assistant” capable of performing\\ntasks typically assigned to humans such as coding large groups of documents,\\nor performing literature reviews. Finally, I assess whether Generative AI will\\nincrease access to programming skills among social scientists, and perhaps even\\nassist them in generating novel or untested hypotheses as well.\\n\\nIn the second section of this article I turn to the various risks and potential\\ndangers associated with Generative AI. On March 29th, 2023, several thousand\\nleading experts in artificial intelligence, computer science, public policy, and many\\nother fields signed an open letter calling for a pause in the development of new\\nGenerative AI models.[3] Signatories of this letter had diverse motivations ranging\\nfrom concerns about how malicious actors might use Generative AI to launch\\ninfluence campaigns on social media to broader concerns about how such tools\\nmight amplify social inequalities, or create new forms of social stratification by\\neliminating jobs typically performed by humans.[4] Others worry that Generative\\nAI can not only produce inaccurate or misleading responses to human questions,\\nbut deliver them with a degree of confidence that might deflect criticism or\\nscrutiny.[5] To these concerns I add that we do not yet know whether or how\\nGenerative AI should be used in research settings due to a lack of transparency\\nin how they are trained, tested, and deployed. I hope this article will provoke\\na conversation among social scientists, computer scientists, ethicists, and AI\\nengineers about how research can be leveraged to identify the promises and\\npitfalls of these techniques.\\n\\nThe most natural place for this conversation to occur might be the nascent field\\nof computational social science— which leverages tools from data science and\\nmachine learning to develop theories of human behavior using the increasingly\\nvoluminous amount of data generated online each day (Edelmann et al. 2020;\\nLazer et al. 2020; Salganik 2018). Computational social science has already\\nexperienced its own share of ethical controversies— long before the advent of\\nGenerative AI (e.g. Fiesler and Proferes 2018; Salganik 2018). Excitement\\nabout the ability to embed experiments within online ecosystems has inspired\\nresearchers to press ahead with research designs that have been criticized for\\nthreatening the safety of internet users in authoritarian regimes (Burnett and\\nFeamster 2015), violating user privacy (Lewis et al. 2008), and enrolling people in\\nonline experiments without their consent (Kramer, Guillory, and Hancock 2014).\\nThese concerning developments within the field most likely to adopt generative\\nAI suggest there may be a range of “unknown unknowns” that will require careful\\nreflection and patience despite the ever increasing pace of publication in this\\nspace.\\n\\n2\\n\\n\\x0cSeveral caveats are in order before I proceed to discuss the issues described\\nabove. First, my analysis of how generative AI might transform research is\\nstrictly limited to social science and thus does not engage with the many different\\nways this technology might shape other fields. Second, the field of generative AI\\nresearch is changing so rapidly that any attempt to take stock of its potential\\nwill become out of date quickly—as well as information about its possible risks or\\ndangers. Therefore, I urge the reader to take caution in evaluating the potential\\nof the research techniques described below, which may yet be judged scientifically\\nunsound, unethical, or both. Instead of a “user’s guide” for generative AI in social\\nscience research, I aim to provoke a broader conversation among researchers\\nabout whether or how these new technologies might be used to study human\\nbehavior in diverse settings.\\n\\nWhat is Generative AI?\\n\\nThe term “generative AI” refers to a broad range of tools developed by researchers\\nin statistics, computer science, and engineering. At a high level, the term\\ndemarcates a shift in the use of machine learning technology from pattern\\nrecognition— where tools are created to identify latent patterns in text, images,\\nor other unstructured datasets— towards the generation of free-form text, images,\\nvideo, and other heretofore human outputs by an algorithm that is trained on vast\\namounts of such data.[6] Large Language Models (LLM) such as ChatGPT ingest\\nvast amounts of text-based data, and identify the probability that a word (or set\\nof words) will occur given the presence of other language patterns within a passage\\nof text. As technology progressed to allow artificial intelligence researchers to\\ntrain such models on increasingly large amounts of text, technologies such as\\nGPT-3 became increasingly adept at predicting the language most likely to follow\\ndifferent “prompts”—short pieces of text designed to shape the LLM’s outputs,\\nsuch as a question. LLM’s thus resemble the “auto-complete” technologies that\\nhave become pervasive on search engines, apps, and other digital spaces over the\\npast decade, but with considerably greater scale and more sophisticated training\\nprocesses.\\n\\nParallel advancements have been made with image— and to a lesser extent video.\\nInstead of calculating the probability of words given other words, generative AI\\ntools that create de novo images use the co-occurence of pixels of different colors\\nor sizes to weave together a range of synthetic visuals. These include synthetic\\nhuman faces, reproductions of classic artwork, or surreal— and at times quite\\ninnovative— new forms of art that have provoked both excitement and concern\\namong people in creative industries. Finally, a new class of models such as\\nDALL-E and Stable Diffusion create such visual content through text prompts—\\nsearching for connections between patterns in the co-occurence of words and the\\narrangement of pixels— that allow a user to request highly specialized visual\\ncontent (such as a picture of Daniel Kahneman riding an elephant across the\\nPrinceton University campus).\\n\\n3\\n\\n\\x0c'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/yourusername/AutoSurveyGPT.git\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:53:16.233686500Z",
     "start_time": "2023-10-12T10:53:16.217541200Z"
    }
   },
   "id": "46735563203829a2"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2+2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T09:35:23.632543400Z",
     "start_time": "2023-10-13T09:35:23.597211900Z"
    }
   },
   "id": "2d79463a365bb712"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = pd.read_csv(\"results.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T09:35:25.937172500Z",
     "start_time": "2023-10-13T09:35:25.240362700Z"
    }
   },
   "id": "a5532c3de650b8f1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import openai\n",
    "types = list(results['Simulation Type'].dropna())\n",
    "benefits = list(results['Simulation Benefits'].dropna())\n",
    "openai.api_key = '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'\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T09:40:50.162563800Z",
     "start_time": "2023-10-13T09:40:50.123203800Z"
    }
   },
   "id": "df934684a4f83034"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo-16k\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\",\n",
    "         \"content\": \"HOw is it going\"}\n",
    "    ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T09:45:48.123169300Z",
     "start_time": "2023-10-13T09:45:44.297968Z"
    }
   },
   "id": "3ee0908946a74499"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "<OpenAIObject at 0xa1c310> JSON: {\n  \"index\": 0,\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": \"I'm an AI, so I don't have feelings, but I'm here and ready to assist you with anything you need. How can I assist you today?\"\n  },\n  \"finish_reason\": \"stop\"\n}"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T09:46:11.665216100Z",
     "start_time": "2023-10-13T09:46:11.646355Z"
    }
   },
   "id": "ceef01391ace0e0d"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "0                                                  NaN\n1                                                  NaN\n2                                                  NaN\n3                                                  NaN\n4                                                  NaN\n5                                                  NaN\n6                                                  NaN\n7    The primary application of LLM in this social ...\nName: Simulation Benefits, dtype: object"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['Simulation Benefits']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T09:48:16.806408900Z",
     "start_time": "2023-10-13T09:48:16.769457200Z"
    }
   },
   "id": "a38bf64a9d806bad"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def generate_answer(question, insights):\n",
    "    # This function will use ChatGPT to generate an answer based on the question and insights\n",
    "    prompt = f\"{question}\\n\\nTo answers use this information and nothing else: {', '.join(insights)}\\n\\n\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\",\n",
    "             \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_report(types,  benefits):\n",
    "    print(\"\\nReport on LLMs in Social Simulations:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    questions = [\n",
    "        \"What are the main types of applications of Social Simulations created using LLMs?\",\n",
    "        \"What distinct benefits do LLMs offer over traditional methods in Social Simulations?\"\n",
    "    ]\n",
    "\n",
    "    insights_lists = [types,  benefits]\n",
    "\n",
    "    for i, question in enumerate(questions):\n",
    "        answer = generate_answer(question, insights_lists[i])\n",
    "        print(f\"\\n{i + 1}. {question}\\n{answer}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T09:47:17.282341900Z",
     "start_time": "2023-10-13T09:47:17.259929500Z"
    }
   },
   "id": "c7522ecf49473c26"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Report on LLMs in Social Simulations:\n",
      "----------------------------------------\n",
      "\n",
      "1. What are the main types of applications of Social Simulations created using LLMs?\n",
      "The main types of applications of Social Simulations created using LLMs include voting simulations.\n",
      "\n",
      "2. What distinct benefits do LLMs offer over traditional methods in Social Simulations?\n",
      "The distinct benefits of using LLMs in social simulations over traditional methods include:\n",
      "\n",
      "1. Improved understanding of deliberation processes: LLMs in social simulations allow researchers to study and analyze the interplay between deliberation processes and voting outcomes. This provides a deeper understanding of how different deliberation mechanisms can affect voting outcomes.\n",
      "\n",
      "2. Enhanced welfare and representation guarantees: The use of LLMs shows that deliberation generally leads to improved welfare and representation guarantees. This means that decision-making processes that involve deliberation have the potential to be more beneficial and representative for the participants involved.\n",
      "\n",
      "3. Identifying effective voting rules: LLMs help in experimentally demonstrating the effectiveness of different voting rules. The researchers find that simple voting rules, like approval voting, can perform as well as more sophisticated rules if deliberation is properly supported. This knowledge can inform the design of voting processes to ensure fair and effective decision-making.\n",
      "\n",
      "4. Proportional voting for fairness: The researchers explore the use of proportional voting rules in fair ranking algorithms and find that they deliver exceptional performance in terms of fairness and relevance metrics. This suggests that proportional voting rules can be valuable in citizen-focused democratic processes and other domains where fairness is a priority.\n",
      "\n",
      "Overall, LLMs offer the advantage of providing insights into the impact of different deliberation mechanisms and voting rules on welfare, representation, fairness, and relevance metrics. This knowledge can inform the design of decision-making processes, improve democratic practices, and prioritize fairness in various domains.\n"
     ]
    }
   ],
   "source": [
    "generate_report(types,  benefits)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T09:47:49.249520900Z",
     "start_time": "2023-10-13T09:47:17.394424Z"
    }
   },
   "id": "3346b6448ff88f06"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "FreeProxyException",
     "evalue": "Request to https://www.sslproxies.org failed",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mgaierror\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\urllib3\\connection.py:159\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 159\u001B[0m     conn \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39mcreate_connection(\n\u001B[0;32m    160\u001B[0m         (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dns_host, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mport), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtimeout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mextra_kw\n\u001B[0;32m    161\u001B[0m     )\n\u001B[0;32m    163\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SocketTimeout:\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\urllib3\\util\\connection.py:61\u001B[0m, in \u001B[0;36mcreate_connection\u001B[1;34m(address, timeout, source_address, socket_options)\u001B[0m\n\u001B[0;32m     59\u001B[0m family \u001B[38;5;241m=\u001B[39m allowed_gai_family()\n\u001B[1;32m---> 61\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetaddrinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfamily\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msocket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSOCK_STREAM\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m     62\u001B[0m     af, socktype, proto, canonname, sa \u001B[38;5;241m=\u001B[39m res\n",
      "File \u001B[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\socket.py:954\u001B[0m, in \u001B[0;36mgetaddrinfo\u001B[1;34m(host, port, family, type, proto, flags)\u001B[0m\n\u001B[0;32m    953\u001B[0m addrlist \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m--> 954\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[43m_socket\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgetaddrinfo\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhost\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfamily\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mproto\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[0;32m    955\u001B[0m     af, socktype, proto, canonname, sa \u001B[38;5;241m=\u001B[39m res\n",
      "\u001B[1;31mgaierror\u001B[0m: [Errno 11002] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mNewConnectionError\u001B[0m                        Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:670\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    669\u001B[0m \u001B[38;5;66;03m# Make the request on the httplib connection object.\u001B[39;00m\n\u001B[1;32m--> 670\u001B[0m httplib_response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    671\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    672\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    673\u001B[0m \u001B[43m    \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    674\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_obj\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    675\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    676\u001B[0m \u001B[43m    \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    677\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    678\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    680\u001B[0m \u001B[38;5;66;03m# If we're going to release the connection in ``finally:``, then\u001B[39;00m\n\u001B[0;32m    681\u001B[0m \u001B[38;5;66;03m# the response doesn't need to know about the connection. Otherwise\u001B[39;00m\n\u001B[0;32m    682\u001B[0m \u001B[38;5;66;03m# it will also try to release it and we'll have a double-release\u001B[39;00m\n\u001B[0;32m    683\u001B[0m \u001B[38;5;66;03m# mess.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:381\u001B[0m, in \u001B[0;36mHTTPConnectionPool._make_request\u001B[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001B[0m\n\u001B[0;32m    380\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 381\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    382\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (SocketTimeout, BaseSSLError) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    383\u001B[0m     \u001B[38;5;66;03m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:978\u001B[0m, in \u001B[0;36mHTTPSConnectionPool._validate_conn\u001B[1;34m(self, conn)\u001B[0m\n\u001B[0;32m    977\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(conn, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msock\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):  \u001B[38;5;66;03m# AppEngine might not have  `.sock`\u001B[39;00m\n\u001B[1;32m--> 978\u001B[0m     \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    980\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m conn\u001B[38;5;241m.\u001B[39mis_verified:\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\urllib3\\connection.py:309\u001B[0m, in \u001B[0;36mHTTPSConnection.connect\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconnect\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    308\u001B[0m     \u001B[38;5;66;03m# Add certificate verification\u001B[39;00m\n\u001B[1;32m--> 309\u001B[0m     conn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_new_conn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    310\u001B[0m     hostname \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhost\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\urllib3\\connection.py:171\u001B[0m, in \u001B[0;36mHTTPConnection._new_conn\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    170\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m SocketError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 171\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NewConnectionError(\n\u001B[0;32m    172\u001B[0m         \u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to establish a new connection: \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m e\n\u001B[0;32m    173\u001B[0m     )\n\u001B[0;32m    175\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m conn\n",
      "\u001B[1;31mNewConnectionError\u001B[0m: <urllib3.connection.HTTPSConnection object at 0x000000001CCF9220>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mMaxRetryError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\requests\\adapters.py:486\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    485\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 486\u001B[0m     resp \u001B[38;5;241m=\u001B[39m \u001B[43mconn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murlopen\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    487\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    488\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    489\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbody\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    490\u001B[0m \u001B[43m        \u001B[49m\u001B[43mheaders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    491\u001B[0m \u001B[43m        \u001B[49m\u001B[43mredirect\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    492\u001B[0m \u001B[43m        \u001B[49m\u001B[43massert_same_host\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    493\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreload_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    494\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdecode_content\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    495\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchunked\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchunked\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (ProtocolError, \u001B[38;5;167;01mOSError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:726\u001B[0m, in \u001B[0;36mHTTPConnectionPool.urlopen\u001B[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001B[0m\n\u001B[0;32m    724\u001B[0m     e \u001B[38;5;241m=\u001B[39m ProtocolError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mConnection aborted.\u001B[39m\u001B[38;5;124m\"\u001B[39m, e)\n\u001B[1;32m--> 726\u001B[0m retries \u001B[38;5;241m=\u001B[39m \u001B[43mretries\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mincrement\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    727\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43me\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_pool\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_stacktrace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msys\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexc_info\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m    728\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    729\u001B[0m retries\u001B[38;5;241m.\u001B[39msleep()\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\urllib3\\util\\retry.py:446\u001B[0m, in \u001B[0;36mRetry.increment\u001B[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001B[0m\n\u001B[0;32m    445\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_retry\u001B[38;5;241m.\u001B[39mis_exhausted():\n\u001B[1;32m--> 446\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MaxRetryError(_pool, url, error \u001B[38;5;129;01mor\u001B[39;00m ResponseError(cause))\n\u001B[0;32m    448\u001B[0m log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIncremented Retry for (url=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m): \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, url, new_retry)\n",
      "\u001B[1;31mMaxRetryError\u001B[0m: HTTPSConnectionPool(host='www.sslproxies.org', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000000001CCF9220>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\fp\\fp.py:32\u001B[0m, in \u001B[0;36mFreeProxy.get_proxy_list\u001B[1;34m(self, repeat)\u001B[0m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m---> 32\u001B[0m     page \u001B[38;5;241m=\u001B[39m \u001B[43mrequests\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__website\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepeat\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m     doc \u001B[38;5;241m=\u001B[39m lh\u001B[38;5;241m.\u001B[39mfromstring(page\u001B[38;5;241m.\u001B[39mcontent)\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\requests\\api.py:73\u001B[0m, in \u001B[0;36mget\u001B[1;34m(url, params, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \n\u001B[0;32m     65\u001B[0m \u001B[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     70\u001B[0m \u001B[38;5;124;03m:rtype: requests.Response\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m---> 73\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m request(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mget\u001B[39m\u001B[38;5;124m\"\u001B[39m, url, params\u001B[38;5;241m=\u001B[39mparams, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\requests\\api.py:59\u001B[0m, in \u001B[0;36mrequest\u001B[1;34m(method, url, **kwargs)\u001B[0m\n\u001B[0;32m     58\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m sessions\u001B[38;5;241m.\u001B[39mSession() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m session\u001B[38;5;241m.\u001B[39mrequest(method\u001B[38;5;241m=\u001B[39mmethod, url\u001B[38;5;241m=\u001B[39murl, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\requests\\sessions.py:589\u001B[0m, in \u001B[0;36mSession.request\u001B[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[0m\n\u001B[0;32m    588\u001B[0m send_kwargs\u001B[38;5;241m.\u001B[39mupdate(settings)\n\u001B[1;32m--> 589\u001B[0m resp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msend(prep, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39msend_kwargs)\n\u001B[0;32m    591\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\requests\\sessions.py:703\u001B[0m, in \u001B[0;36mSession.send\u001B[1;34m(self, request, **kwargs)\u001B[0m\n\u001B[0;32m    702\u001B[0m \u001B[38;5;66;03m# Send the request\u001B[39;00m\n\u001B[1;32m--> 703\u001B[0m r \u001B[38;5;241m=\u001B[39m adapter\u001B[38;5;241m.\u001B[39msend(request, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    705\u001B[0m \u001B[38;5;66;03m# Total elapsed time of the request (approximately)\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\requests\\adapters.py:519\u001B[0m, in \u001B[0;36mHTTPAdapter.send\u001B[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001B[0m\n\u001B[0;32m    517\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m SSLError(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[1;32m--> 519\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(e, request\u001B[38;5;241m=\u001B[39mrequest)\n\u001B[0;32m    521\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m ClosedPoolError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[1;31mConnectionError\u001B[0m: HTTPSConnectionPool(host='www.sslproxies.org', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x000000001CCF9220>: Failed to establish a new connection: [Errno 11002] getaddrinfo failed'))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mFreeProxyException\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 6\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpprint\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pprint\n\u001B[0;32m      5\u001B[0m pg \u001B[38;5;241m=\u001B[39m ProxyGenerator()\n\u001B[1;32m----> 6\u001B[0m \u001B[43mpg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFreeProxies\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m scholarly\u001B[38;5;241m.\u001B[39muse_proxy(pg)\n\u001B[0;32m      8\u001B[0m scholarly\u001B[38;5;241m.\u001B[39msearch_pubs(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLarge Batch Optimization for Deep Learning: Training BERT in 76 minutes\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\_proxy_generator.py:550\u001B[0m, in \u001B[0;36mProxyGenerator.FreeProxies\u001B[1;34m(self, timeout, wait_time)\u001B[0m\n\u001B[0;32m    548\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp_gen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp_coroutine(timeout\u001B[38;5;241m=\u001B[39mtimeout, wait_time\u001B[38;5;241m=\u001B[39mwait_time)\n\u001B[0;32m    549\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_proxy_gen \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fp_gen\u001B[38;5;241m.\u001B[39msend\n\u001B[1;32m--> 550\u001B[0m proxy \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_proxy_gen\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# prime the generator\u001B[39;00m\n\u001B[0;32m    551\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlogger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrying with proxy \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, proxy)\n\u001B[0;32m    552\u001B[0m proxy_works \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_use_proxy(proxy)\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\_proxy_generator.py:509\u001B[0m, in \u001B[0;36mProxyGenerator._fp_coroutine\u001B[1;34m(self, timeout, wait_time)\u001B[0m\n\u001B[0;32m    507\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dirty_freeproxies \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n\u001B[0;32m    508\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 509\u001B[0m     all_proxies \u001B[38;5;241m=\u001B[39m \u001B[43mfreeproxy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_proxy_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrepeat\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# free-proxy >= 1.1.0\u001B[39;00m\n\u001B[0;32m    510\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    511\u001B[0m     all_proxies \u001B[38;5;241m=\u001B[39m freeproxy\u001B[38;5;241m.\u001B[39mget_proxy_list()  \u001B[38;5;66;03m# free-proxy < 1.1.0\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\fp\\fp.py:35\u001B[0m, in \u001B[0;36mFreeProxy.get_proxy_list\u001B[1;34m(self, repeat)\u001B[0m\n\u001B[0;32m     33\u001B[0m     doc \u001B[38;5;241m=\u001B[39m lh\u001B[38;5;241m.\u001B[39mfromstring(page\u001B[38;5;241m.\u001B[39mcontent)\n\u001B[0;32m     34\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m requests\u001B[38;5;241m.\u001B[39mexceptions\u001B[38;5;241m.\u001B[39mRequestException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m---> 35\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m FreeProxyException(\n\u001B[0;32m     36\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRequest to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__website(repeat)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m failed\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     38\u001B[0m     tr_elements \u001B[38;5;241m=\u001B[39m doc\u001B[38;5;241m.\u001B[39mxpath(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m//*[@id=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlist\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m]//tr\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mFreeProxyException\u001B[0m: Request to https://www.sslproxies.org failed"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from scholarly import scholarly,ProxyGenerator\n",
    "from pprint import pprint\n",
    "\n",
    "pg = ProxyGenerator()\n",
    "pg.FreeProxies()\n",
    "scholarly.use_proxy(pg)\n",
    "scholarly.search_pubs('Large Batch Optimization for Deep Learning: Training BERT in 76 minutes')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-13T09:26:39.062267700Z",
     "start_time": "2023-10-13T09:26:27.136038900Z"
    }
   },
   "id": "51589d7931f01521"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "\n",
    "# Retrieve the author's data, fill-in, and print\n",
    "# Get an iterator for the author results\n",
    "search_query = scholarly.search_author('( \"social simulation\" OR \"social modelling\" ) AND ( \"Large Language Models\" OR \"LLMs\" ) AND \"Computational Social Science\"')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T11:02:12.232620600Z",
     "start_time": "2023-10-12T11:02:10.734475800Z"
    }
   },
   "id": "27355d3530e593e0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "MaxTriesExceededException",
     "evalue": "Cannot Fetch from Google Scholar.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMaxTriesExceededException\u001B[0m                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m search_query \u001B[38;5;241m=\u001B[39m \u001B[43mscholarly\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch_pubs\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msimulation\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m scholarly\u001B[38;5;241m.\u001B[39mpprint(\u001B[38;5;28mnext\u001B[39m(search_query))\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\_scholarly.py:160\u001B[0m, in \u001B[0;36m_Scholarly.search_pubs\u001B[1;34m(self, query, patents, citations, year_low, year_high, sort_by, include_last_year, start_index)\u001B[0m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Searches by query and returns a generator of Publication objects\u001B[39;00m\n\u001B[0;32m     98\u001B[0m \n\u001B[0;32m     99\u001B[0m \u001B[38;5;124;03m:param query: terms to be searched\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    155\u001B[0m \n\u001B[0;32m    156\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    157\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_construct_url(_PUBSEARCH\u001B[38;5;241m.\u001B[39mformat(requests\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mquote(query)), patents\u001B[38;5;241m=\u001B[39mpatents,\n\u001B[0;32m    158\u001B[0m                           citations\u001B[38;5;241m=\u001B[39mcitations, year_low\u001B[38;5;241m=\u001B[39myear_low, year_high\u001B[38;5;241m=\u001B[39myear_high,\n\u001B[0;32m    159\u001B[0m                           sort_by\u001B[38;5;241m=\u001B[39msort_by, include_last_year\u001B[38;5;241m=\u001B[39minclude_last_year, start_index\u001B[38;5;241m=\u001B[39mstart_index)\n\u001B[1;32m--> 160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__nav\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch_publications\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\_navigator.py:296\u001B[0m, in \u001B[0;36mNavigator.search_publications\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msearch_publications\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _SearchScholarIterator:\n\u001B[0;32m    289\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns a Publication Generator given a url\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \n\u001B[0;32m    291\u001B[0m \u001B[38;5;124;03m    :param url: the url where publications can be found.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    294\u001B[0m \u001B[38;5;124;03m    :rtype: {_SearchScholarIterator}\u001B[39;00m\n\u001B[0;32m    295\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 296\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_SearchScholarIterator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\publication_parser.py:53\u001B[0m, in \u001B[0;36m_SearchScholarIterator.__init__\u001B[1;34m(self, nav, url)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pubtype \u001B[38;5;241m=\u001B[39m PublicationSource\u001B[38;5;241m.\u001B[39mPUBLICATION_SEARCH_SNIPPET \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/scholar?\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m url \u001B[38;5;28;01melse\u001B[39;00m PublicationSource\u001B[38;5;241m.\u001B[39mJOURNAL_CITATION_LIST\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_nav \u001B[38;5;241m=\u001B[39m nav\n\u001B[1;32m---> 53\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_total_results()\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpub_parser \u001B[38;5;241m=\u001B[39m PublicationParser(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_nav)\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\publication_parser.py:59\u001B[0m, in \u001B[0;36m_SearchScholarIterator._load_url\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_url\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;66;03m# this is temporary until setup json file\u001B[39;00m\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nav\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_soup\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pos \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m'\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgs_r gs_or gs_scl\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m'\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgsc_mpat_ttl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\_navigator.py:239\u001B[0m, in \u001B[0;36mNavigator._get_soup\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_soup\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BeautifulSoup:\n\u001B[0;32m    238\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001B[39;00m\n\u001B[1;32m--> 239\u001B[0m     html \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_page\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhttps://scholar.google.com\u001B[39;49m\u001B[38;5;132;43;01m{0}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    240\u001B[0m     html \u001B[38;5;241m=\u001B[39m html\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\xa0\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    241\u001B[0m     res \u001B[38;5;241m=\u001B[39m BeautifulSoup(html, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\_navigator.py:190\u001B[0m, in \u001B[0;36mNavigator._get_page\u001B[1;34m(self, pagerequest, premium)\u001B[0m\n\u001B[0;32m    188\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_page(pagerequest, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 190\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MaxTriesExceededException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot Fetch from Google Scholar.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mMaxTriesExceededException\u001B[0m: Cannot Fetch from Google Scholar."
     ]
    }
   ],
   "source": [
    "search_query = scholarly.search_pubs('simulation')\n",
    "scholarly.pprint(next(search_query))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:06:05.638914200Z",
     "start_time": "2023-10-11T16:05:54.703752500Z"
    }
   },
   "id": "c3a1c66e17c18ed1"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"{'affiliation': 'Chubu University',\\n 'citedby': 240049,\\n 'email_domain': '@slac.stanford.edu',\\n 'filled': False,\\n 'interests': ['Simulation', 'High Energy Physics', 'Cosmic Ray Physics'],\\n 'name': 'Tatsumi Koi',\\n 'scholar_id': 'CsO0RukAAAAJ',\\n 'source': 'SEARCH_AUTHOR_SNIPPETS',\\n 'url_picture': 'https://scholar.google.com/citations?view_op=medium_photo&user=CsO0RukAAAAJ'}\"\n"
     ]
    }
   ],
   "source": [
    "search_query = scholarly.search_keyword('simulation')\n",
    "scholarly.pprint(next(search_query))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T13:11:02.784992400Z",
     "start_time": "2023-10-11T13:11:00.903741700Z"
    }
   },
   "id": "7fdc74465345e0e9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"{'affiliation': 'Danish Broadcasting Corporation, PhD Niels Bohr Institute',\\n 'citedby': 122102,\\n 'email_domain': '@nbi.dk',\\n 'filled': False,\\n 'interests': ['Particle physics',\\n               'multivariate methods',\\n               'simulation',\\n               'outreach',\\n               'innovation'],\\n 'name': 'Ask Emil Loevschall-Jensen',\\n 'scholar_id': 'PzROYKsAAAAJ',\\n 'source': 'SEARCH_AUTHOR_SNIPPETS',\\n 'url_picture': 'https://scholar.google.com/citations?view_op=medium_photo&user=PzROYKsAAAAJ'}\"\n"
     ]
    }
   ],
   "source": [
    "scholarly.pprint(next(search_query))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T12:59:42.219446100Z",
     "start_time": "2023-10-11T12:59:42.200266200Z"
    }
   },
   "id": "e687ffc52f7efb23"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import logging\n",
    "import configparser\n",
    "import time\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:49:05.873633900Z",
     "start_time": "2023-10-06T14:49:04.884680400Z"
    }
   },
   "id": "1590d8d46ca36854"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "# Basic configs:\n",
    "logging.basicConfig(filename='log.log', level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# Load PICOC terms\n",
    "picoc = {\n",
    "    'population': re.compile(config['picoc']['population']) if config['picoc']['population'] != \"\" else None,\n",
    "    'intervention': re.compile(config['picoc']['intervention']) if config['picoc']['intervention'] != \"\" else None,\n",
    "    'comparison': re.compile(config['picoc']['comparison']) if config['picoc']['comparison'] != \"\" else None,\n",
    "    'outcome': re.compile(config['picoc']['outcome']) if config['picoc']['outcome'] != \"\" else None,\n",
    "    'context': re.compile(config['picoc']['context']) if config['picoc']['context'] != \"\" else None\n",
    "}\n",
    "\n",
    "# Create a new Chorme session\n",
    "\n",
    "options = None\n",
    "if config['default']['binary_location']:\n",
    "    options = Options()\n",
    "    options.binary_location = config['default']['binary_location']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:49:35.865688300Z",
     "start_time": "2023-10-06T14:49:35.846726100Z"
    }
   },
   "id": "305178bcefe4865f"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\karet\\Downloads\\chromedriver_win32\\chromedriver\", options=options)\n",
    "url = \"https://scholar.google.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Setting Google Scholar\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "driver.find_element_by_id(\"gs_hdr_mnu\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_class_name(\"gs_btnP\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_id(\"gs_num-b\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_css_selector('a[data-v=\"20\"').click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_id(\"gs_settings_import_some\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_name(\"save\").click()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:52:52.706419400Z",
     "start_time": "2023-10-06T14:52:35.576492500Z"
    }
   },
   "id": "cd101360d9d70244"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "query = '( \"social simulation\" OR \"social modelling\" ) AND ( \"Large Language Models\" OR \"LLMs\" ) AND \"Computational Social Science\"'\n",
    "year= 2023\n",
    "\n",
    "\n",
    "driver.get(url + \"scholar?hl=en&q={0}&as_sdt=1&as_vis=1&as_ylo={1}&as_yhi={1}\".format(query, year))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:53:39.246704600Z",
     "start_time": "2023-10-06T14:53:38.349239900Z"
    }
   },
   "id": "d817dba1127f9b8d"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def parser(soup, page, year):\n",
    "    papers = []\n",
    "    html = soup.findAll('div', {'class': 'gs_r gs_or gs_scl'})\n",
    "    for result in html:\n",
    "        paper = {'Link': result.find('h3', {'class': \"gs_rt\"}).find('a')['href'], 'Additional link': '', 'Title': '',\n",
    "                 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '',\n",
    "                 'Year': year, 'Google page': page}\n",
    "\n",
    "        # If it does not pass at Title-Abstract-Keyword filter exclude this paper and continue\n",
    "        if not filterTitleAbsKey(paper['Link']):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            paper[\"Additional link\"] = result.find('div', {'class': \"gs_or_ggsm\"}).find('a')['href']\n",
    "        except:\n",
    "            paper[\"Additional link\"] = ''\n",
    "            print(\"NOTHING WAS FOUND\")\n",
    "\n",
    "        paper['Title'] = result.find('h3', {'class': \"gs_rt\"}).text\n",
    "        paper['Authors'] = \";\".join(\n",
    "            [\"%s:%s\" % (a.text, a['href']) for a in result.find('div', {'class': \"gs_a\"}).findAll('a')])\n",
    "\n",
    "        try:\n",
    "            paper['Abstract'] = result.find('div', {'class': \"gs_rs\"}).text\n",
    "        except:\n",
    "            print(\"NOTHING WAS FOUND\")\n",
    "            paper['Abstract'] = ''\n",
    "\n",
    "        for a in result.findAll('div', {'class': \"gs_fl\"})[-1].findAll('a'):\n",
    "            if a.text != '':\n",
    "                if a.text.startswith('Cited'):\n",
    "                    paper['Cited by'] = a.text.rstrip().split()[-1]\n",
    "                    paper['Cited list'] = url + a['href']\n",
    "                if a.text.startswith('Related'):\n",
    "                    paper['Related list'] = url + a['href']\n",
    "                if a.text.startswith('Import'):\n",
    "                    paper['Bibtex'] = requests.get(a['href']).text\n",
    "                    \n",
    "        papers.append(paper)\n",
    "        # Wait 20 seconds until the next request to google\n",
    "        time.sleep(20)\n",
    "\n",
    "    return papers, len(html)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:55:27.836002300Z",
     "start_time": "2023-10-06T14:55:27.797322500Z"
    }
   },
   "id": "123da238874b0ad7"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def filterTitleAbsKey(site):\n",
    "    try:\n",
    "        page = requests.get(site,timeout=600)\n",
    "        text = BeautifulSoup(page.text, 'lxml').get_text()\n",
    "        text = str.lower(text)\n",
    "        for terms in filter(None, picoc.values()):\n",
    "            if not terms.search(text):\n",
    "                logging.info(\"%s not passed on title-abs-key filter\", site)\n",
    "                return False\n",
    "        logging.info(\"%s passed on title-abs-key filter\", site)\n",
    "        return True\n",
    "    except requests.exceptions.Timeout:\n",
    "        logging.info(\"[TIMEOUT] Timeout on %s and not passed on title-abs-key filter. Skipping website\", site)\n",
    "    except:\n",
    "        logging.info(\"[ERROR] on %s and not passed on title-abs-key filter\", site)\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:55:29.079586700Z",
     "start_time": "2023-10-06T14:55:29.067266400Z"
    }
   },
   "id": "19f24f1eab272dfd"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "art, t = parser(BeautifulSoup(driver.page_source, 'lxml'), 1, 2023)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:55:53.563694100Z",
     "start_time": "2023-10-06T14:55:29.458456100Z"
    }
   },
   "id": "1db24912d2a11345"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "html = BeautifulSoup(driver.page_source, 'lxml').findAll('div', {'class': 'gs_r gs_or gs_scl'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:56:52.101914600Z",
     "start_time": "2023-10-06T14:56:52.062986400Z"
    }
   },
   "id": "1510bad62586863c"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Link': 'https://osf.io/rwtzs/download', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n",
      "{'Link': 'https://arxiv.org/abs/2307.14984', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n",
      "{'Link': 'https://arxiv.org/abs/2308.11432', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n",
      "{'Link': 'https://search.proquest.com/openview/0bbaae522618c888853a4c6dbfa6a58e/1?pq-origsite=gscholar&cbl=18750&diss=y', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n",
      "{'Link': 'https://research-information.bris.ac.uk/files/364550531/FINAL_DRAFT_1.pdf', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n",
      "{'Link': 'https://arxiv.org/abs/2306.13723', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n",
      "{'Link': 'https://uwspace.uwaterloo.ca/handle/10012/19713', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n"
     ]
    }
   ],
   "source": [
    "for result in html:\n",
    "    paper = {'Link': result.find('h3', {'class': \"gs_rt\"}).find('a')['href'], 'Additional link': '', 'Title': '',\n",
    "             'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '',\n",
    "             'Year': year, 'Google page': 1}\n",
    "    print(paper)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:57:32.929912700Z",
     "start_time": "2023-10-06T14:57:32.915827500Z"
    }
   },
   "id": "baec9a7eb170b62f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "45cbeecd53c63e8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
