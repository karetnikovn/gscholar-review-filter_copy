{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:51:46.732751400Z",
     "start_time": "2023-10-12T10:51:37.171459200Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install scholarly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "!pip3 install -U scholarly\n",
    "!sudo apt-get install tor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:53:16.215026Z",
     "start_time": "2023-10-12T10:53:13.958529300Z"
    }
   },
   "id": "33c67ba2d2e9f3ff"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "'Can Generative AI Improve Social Science Research?[1]\\n\\nChris Bail\\nDuke University\\nwww.chrisbail.net\\n\\nAbstract. Artificial intelligence that can produce realistic text, images, and\\nother human-like outputs is currently transforming many different industries.\\nYet it is not yet known how such tools might transform social science research. In\\nthe first section of this article, I assess the potential of Generative AI to improve\\nonline experiments, agent-based models, and automated content analyses. I also\\ndiscuss whether these tools may help social scientists perform literature reviews,\\nidentify novel research questions, and develop hypotheses to explain them. Next,\\nI evaluate whether Generative AI can help social scientists with more mundane\\ntasks such as acquiring advanced programming skills or writing more effective\\nprose. In the second section of this article I discuss the limitations of Generative\\nAI as well as how these tools might be employed by researchers in an ethical\\nmanner. I discuss how bias in the processes and data used to train these tools can\\nnegatively impact social science research as well as a range of other challenges\\nrelated to accuracy, reproducibility, interpretability, and efficiency. I conclude by\\nhighlighting the need for increased collaboration between social scientists and\\nartificial intelligence researchers— not only to ensure that such tools are used in\\na safe and ethical manner, but also because the progress of artificial intelligence\\nmay require deeper understanding of theories of human behavior.\\n\\nIntroduction\\n\\nGenerative Artificial Intelligence— technology capable of producing realistic text,\\nimages, music, and other creative forms— continues to captivate large audiences.\\nChatGPT, the conversational chatbot generated by OpenAI, recently became the\\nfastest-growing consumer application in history. According to one report, this tool\\namassed more than 13 million unique users each day in January 2023.[2] There is\\nwidespread speculation that such generative AI will have considerable impact on\\na range of different industries— from creative and legal writing to advertising\\nand customer service. Yet sociologists, political scientists, economists and other\\nsocial scientists are only beginning to explore how generative AI will transform\\ntheir research. In this article, I evaluate whether social scientists can employ\\nGenerative AI to enhance conventional research methods, but also invent entirely\\nnew forms of inquiry as well. Along the way, I evaluate whether the use of such\\ntools is ethical in research settings, and how scholars interested in exploring\\nsuch technologies might mitigate the various risks associated with these largely\\nuntested technologies.\\n\\nIn the first section of this article, I ask whether Generative AI can effectively\\n\\n1\\n\\n\\x0csimulate human behavior for the purposes of social science research. I examine\\nwhether these tools may be useful for creating synthetic human-like content\\nsuch as images used in survey experiments or text for vignette studies. Next,\\nI review recent studies that employ Generative AI models to simulate human\\npopulations taking public opinion surveys, impersonate team-members in online\\nexperiments, and provide more realistic agent-based models. I then ask whether\\nGenerative AI can become a “virtual research assistant” capable of performing\\ntasks typically assigned to humans such as coding large groups of documents,\\nor performing literature reviews. Finally, I assess whether Generative AI will\\nincrease access to programming skills among social scientists, and perhaps even\\nassist them in generating novel or untested hypotheses as well.\\n\\nIn the second section of this article I turn to the various risks and potential\\ndangers associated with Generative AI. On March 29th, 2023, several thousand\\nleading experts in artificial intelligence, computer science, public policy, and many\\nother fields signed an open letter calling for a pause in the development of new\\nGenerative AI models.[3] Signatories of this letter had diverse motivations ranging\\nfrom concerns about how malicious actors might use Generative AI to launch\\ninfluence campaigns on social media to broader concerns about how such tools\\nmight amplify social inequalities, or create new forms of social stratification by\\neliminating jobs typically performed by humans.[4] Others worry that Generative\\nAI can not only produce inaccurate or misleading responses to human questions,\\nbut deliver them with a degree of confidence that might deflect criticism or\\nscrutiny.[5] To these concerns I add that we do not yet know whether or how\\nGenerative AI should be used in research settings due to a lack of transparency\\nin how they are trained, tested, and deployed. I hope this article will provoke\\na conversation among social scientists, computer scientists, ethicists, and AI\\nengineers about how research can be leveraged to identify the promises and\\npitfalls of these techniques.\\n\\nThe most natural place for this conversation to occur might be the nascent field\\nof computational social science— which leverages tools from data science and\\nmachine learning to develop theories of human behavior using the increasingly\\nvoluminous amount of data generated online each day (Edelmann et al. 2020;\\nLazer et al. 2020; Salganik 2018). Computational social science has already\\nexperienced its own share of ethical controversies— long before the advent of\\nGenerative AI (e.g. Fiesler and Proferes 2018; Salganik 2018). Excitement\\nabout the ability to embed experiments within online ecosystems has inspired\\nresearchers to press ahead with research designs that have been criticized for\\nthreatening the safety of internet users in authoritarian regimes (Burnett and\\nFeamster 2015), violating user privacy (Lewis et al. 2008), and enrolling people in\\nonline experiments without their consent (Kramer, Guillory, and Hancock 2014).\\nThese concerning developments within the field most likely to adopt generative\\nAI suggest there may be a range of “unknown unknowns” that will require careful\\nreflection and patience despite the ever increasing pace of publication in this\\nspace.\\n\\n2\\n\\n\\x0cSeveral caveats are in order before I proceed to discuss the issues described\\nabove. First, my analysis of how generative AI might transform research is\\nstrictly limited to social science and thus does not engage with the many different\\nways this technology might shape other fields. Second, the field of generative AI\\nresearch is changing so rapidly that any attempt to take stock of its potential\\nwill become out of date quickly—as well as information about its possible risks or\\ndangers. Therefore, I urge the reader to take caution in evaluating the potential\\nof the research techniques described below, which may yet be judged scientifically\\nunsound, unethical, or both. Instead of a “user’s guide” for generative AI in social\\nscience research, I aim to provoke a broader conversation among researchers\\nabout whether or how these new technologies might be used to study human\\nbehavior in diverse settings.\\n\\nWhat is Generative AI?\\n\\nThe term “generative AI” refers to a broad range of tools developed by researchers\\nin statistics, computer science, and engineering. At a high level, the term\\ndemarcates a shift in the use of machine learning technology from pattern\\nrecognition— where tools are created to identify latent patterns in text, images,\\nor other unstructured datasets— towards the generation of free-form text, images,\\nvideo, and other heretofore human outputs by an algorithm that is trained on vast\\namounts of such data.[6] Large Language Models (LLM) such as ChatGPT ingest\\nvast amounts of text-based data, and identify the probability that a word (or set\\nof words) will occur given the presence of other language patterns within a passage\\nof text. As technology progressed to allow artificial intelligence researchers to\\ntrain such models on increasingly large amounts of text, technologies such as\\nGPT-3 became increasingly adept at predicting the language most likely to follow\\ndifferent “prompts”—short pieces of text designed to shape the LLM’s outputs,\\nsuch as a question. LLM’s thus resemble the “auto-complete” technologies that\\nhave become pervasive on search engines, apps, and other digital spaces over the\\npast decade, but with considerably greater scale and more sophisticated training\\nprocesses.\\n\\nParallel advancements have been made with image— and to a lesser extent video.\\nInstead of calculating the probability of words given other words, generative AI\\ntools that create de novo images use the co-occurence of pixels of different colors\\nor sizes to weave together a range of synthetic visuals. These include synthetic\\nhuman faces, reproductions of classic artwork, or surreal— and at times quite\\ninnovative— new forms of art that have provoked both excitement and concern\\namong people in creative industries. Finally, a new class of models such as\\nDALL-E and Stable Diffusion create such visual content through text prompts—\\nsearching for connections between patterns in the co-occurence of words and the\\narrangement of pixels— that allow a user to request highly specialized visual\\ncontent (such as a picture of Daniel Kahneman riding an elephant across the\\nPrinceton University campus).\\n\\n3\\n\\n\\x0c'"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!git clone https://github.com/yourusername/AutoSurveyGPT.git\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T10:53:16.233686500Z",
     "start_time": "2023-10-12T10:53:16.217541200Z"
    }
   },
   "id": "46735563203829a2"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import random\n",
    "from scholarly import scholarly,ProxyGenerator\n",
    "from pprint import pprint\n",
    "\n",
    "pg = ProxyGenerator()\n",
    "pg.FreeProxies()\n",
    "scholarly.use_proxy(pg)\n",
    "scholarly.search_pubs('Large Batch Optimization for Deep Learning: Training BERT in 76 minutes')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T11:02:10.067206200Z",
     "start_time": "2023-10-12T11:02:10.052283700Z"
    }
   },
   "id": "51589d7931f01521"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scholarly import scholarly\n",
    "\n",
    "# Retrieve the author's data, fill-in, and print\n",
    "# Get an iterator for the author results\n",
    "search_query = scholarly.search_author('( \"social simulation\" OR \"social modelling\" ) AND ( \"Large Language Models\" OR \"LLMs\" ) AND \"Computational Social Science\"')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-12T11:02:12.232620600Z",
     "start_time": "2023-10-12T11:02:10.734475800Z"
    }
   },
   "id": "27355d3530e593e0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "MaxTriesExceededException",
     "evalue": "Cannot Fetch from Google Scholar.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMaxTriesExceededException\u001B[0m                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m search_query \u001B[38;5;241m=\u001B[39m \u001B[43mscholarly\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch_pubs\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msimulation\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m scholarly\u001B[38;5;241m.\u001B[39mpprint(\u001B[38;5;28mnext\u001B[39m(search_query))\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\_scholarly.py:160\u001B[0m, in \u001B[0;36m_Scholarly.search_pubs\u001B[1;34m(self, query, patents, citations, year_low, year_high, sort_by, include_last_year, start_index)\u001B[0m\n\u001B[0;32m     97\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Searches by query and returns a generator of Publication objects\u001B[39;00m\n\u001B[0;32m     98\u001B[0m \n\u001B[0;32m     99\u001B[0m \u001B[38;5;124;03m:param query: terms to be searched\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    155\u001B[0m \n\u001B[0;32m    156\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    157\u001B[0m url \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_construct_url(_PUBSEARCH\u001B[38;5;241m.\u001B[39mformat(requests\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mquote(query)), patents\u001B[38;5;241m=\u001B[39mpatents,\n\u001B[0;32m    158\u001B[0m                           citations\u001B[38;5;241m=\u001B[39mcitations, year_low\u001B[38;5;241m=\u001B[39myear_low, year_high\u001B[38;5;241m=\u001B[39myear_high,\n\u001B[0;32m    159\u001B[0m                           sort_by\u001B[38;5;241m=\u001B[39msort_by, include_last_year\u001B[38;5;241m=\u001B[39minclude_last_year, start_index\u001B[38;5;241m=\u001B[39mstart_index)\n\u001B[1;32m--> 160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__nav\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch_publications\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\_navigator.py:296\u001B[0m, in \u001B[0;36mNavigator.search_publications\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m    288\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msearch_publications\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m _SearchScholarIterator:\n\u001B[0;32m    289\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Returns a Publication Generator given a url\u001B[39;00m\n\u001B[0;32m    290\u001B[0m \n\u001B[0;32m    291\u001B[0m \u001B[38;5;124;03m    :param url: the url where publications can be found.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    294\u001B[0m \u001B[38;5;124;03m    :rtype: {_SearchScholarIterator}\u001B[39;00m\n\u001B[0;32m    295\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 296\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_SearchScholarIterator\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\publication_parser.py:53\u001B[0m, in \u001B[0;36m_SearchScholarIterator.__init__\u001B[1;34m(self, nav, url)\u001B[0m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pubtype \u001B[38;5;241m=\u001B[39m PublicationSource\u001B[38;5;241m.\u001B[39mPUBLICATION_SEARCH_SNIPPET \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/scholar?\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m url \u001B[38;5;28;01melse\u001B[39;00m PublicationSource\u001B[38;5;241m.\u001B[39mJOURNAL_CITATION_LIST\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_nav \u001B[38;5;241m=\u001B[39m nav\n\u001B[1;32m---> 53\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load_url\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtotal_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_total_results()\n\u001B[0;32m     55\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpub_parser \u001B[38;5;241m=\u001B[39m PublicationParser(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_nav)\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\publication_parser.py:59\u001B[0m, in \u001B[0;36m_SearchScholarIterator._load_url\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m     57\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_load_url\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m     58\u001B[0m     \u001B[38;5;66;03m# this is temporary until setup json file\u001B[39;00m\n\u001B[1;32m---> 59\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nav\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_soup\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pos \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     61\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rows \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m'\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgs_r gs_or gs_scl\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_soup\u001B[38;5;241m.\u001B[39mfind_all(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdiv\u001B[39m\u001B[38;5;124m'\u001B[39m, class_\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgsc_mpat_ttl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\_navigator.py:239\u001B[0m, in \u001B[0;36mNavigator._get_soup\u001B[1;34m(self, url)\u001B[0m\n\u001B[0;32m    237\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_get_soup\u001B[39m(\u001B[38;5;28mself\u001B[39m, url: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BeautifulSoup:\n\u001B[0;32m    238\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Return the BeautifulSoup for a page on scholar.google.com\"\"\"\u001B[39;00m\n\u001B[1;32m--> 239\u001B[0m     html \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_page\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mhttps://scholar.google.com\u001B[39;49m\u001B[38;5;132;43;01m{0}\u001B[39;49;00m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[43murl\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    240\u001B[0m     html \u001B[38;5;241m=\u001B[39m html\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;130;01m\\xa0\u001B[39;00m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124mu\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    241\u001B[0m     res \u001B[38;5;241m=\u001B[39m BeautifulSoup(html, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhtml.parser\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\gscholar-review-filter_copy\\venv\\lib\\site-packages\\scholarly\\_navigator.py:190\u001B[0m, in \u001B[0;36mNavigator._get_page\u001B[1;34m(self, pagerequest, premium)\u001B[0m\n\u001B[0;32m    188\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_page(pagerequest, \u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    189\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 190\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m MaxTriesExceededException(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot Fetch from Google Scholar.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mMaxTriesExceededException\u001B[0m: Cannot Fetch from Google Scholar."
     ]
    }
   ],
   "source": [
    "search_query = scholarly.search_pubs('simulation')\n",
    "scholarly.pprint(next(search_query))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T16:06:05.638914200Z",
     "start_time": "2023-10-11T16:05:54.703752500Z"
    }
   },
   "id": "c3a1c66e17c18ed1"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"{'affiliation': 'Chubu University',\\n 'citedby': 240049,\\n 'email_domain': '@slac.stanford.edu',\\n 'filled': False,\\n 'interests': ['Simulation', 'High Energy Physics', 'Cosmic Ray Physics'],\\n 'name': 'Tatsumi Koi',\\n 'scholar_id': 'CsO0RukAAAAJ',\\n 'source': 'SEARCH_AUTHOR_SNIPPETS',\\n 'url_picture': 'https://scholar.google.com/citations?view_op=medium_photo&user=CsO0RukAAAAJ'}\"\n"
     ]
    }
   ],
   "source": [
    "search_query = scholarly.search_keyword('simulation')\n",
    "scholarly.pprint(next(search_query))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T13:11:02.784992400Z",
     "start_time": "2023-10-11T13:11:00.903741700Z"
    }
   },
   "id": "7fdc74465345e0e9"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"{'affiliation': 'Danish Broadcasting Corporation, PhD Niels Bohr Institute',\\n 'citedby': 122102,\\n 'email_domain': '@nbi.dk',\\n 'filled': False,\\n 'interests': ['Particle physics',\\n               'multivariate methods',\\n               'simulation',\\n               'outreach',\\n               'innovation'],\\n 'name': 'Ask Emil Loevschall-Jensen',\\n 'scholar_id': 'PzROYKsAAAAJ',\\n 'source': 'SEARCH_AUTHOR_SNIPPETS',\\n 'url_picture': 'https://scholar.google.com/citations?view_op=medium_photo&user=PzROYKsAAAAJ'}\"\n"
     ]
    }
   ],
   "source": [
    "scholarly.pprint(next(search_query))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-11T12:59:42.219446100Z",
     "start_time": "2023-10-11T12:59:42.200266200Z"
    }
   },
   "id": "e687ffc52f7efb23"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import logging\n",
    "import configparser\n",
    "import time\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:49:05.873633900Z",
     "start_time": "2023-10-06T14:49:04.884680400Z"
    }
   },
   "id": "1590d8d46ca36854"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\n",
    "# Basic configs:\n",
    "logging.basicConfig(filename='log.log', level=logging.INFO, format='%(asctime)s %(message)s')\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "# Load PICOC terms\n",
    "picoc = {\n",
    "    'population': re.compile(config['picoc']['population']) if config['picoc']['population'] != \"\" else None,\n",
    "    'intervention': re.compile(config['picoc']['intervention']) if config['picoc']['intervention'] != \"\" else None,\n",
    "    'comparison': re.compile(config['picoc']['comparison']) if config['picoc']['comparison'] != \"\" else None,\n",
    "    'outcome': re.compile(config['picoc']['outcome']) if config['picoc']['outcome'] != \"\" else None,\n",
    "    'context': re.compile(config['picoc']['context']) if config['picoc']['context'] != \"\" else None\n",
    "}\n",
    "\n",
    "# Create a new Chorme session\n",
    "\n",
    "options = None\n",
    "if config['default']['binary_location']:\n",
    "    options = Options()\n",
    "    options.binary_location = config['default']['binary_location']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:49:35.865688300Z",
     "start_time": "2023-10-06T14:49:35.846726100Z"
    }
   },
   "id": "305178bcefe4865f"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(r\"C:\\Users\\karet\\Downloads\\chromedriver_win32\\chromedriver\", options=options)\n",
    "url = \"https://scholar.google.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "# Setting Google Scholar\n",
    "driver.maximize_window()\n",
    "time.sleep(2)\n",
    "driver.find_element_by_id(\"gs_hdr_mnu\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_class_name(\"gs_btnP\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_id(\"gs_num-b\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_css_selector('a[data-v=\"20\"').click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_id(\"gs_settings_import_some\").click()\n",
    "time.sleep(2)\n",
    "\n",
    "driver.find_element_by_name(\"save\").click()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:52:52.706419400Z",
     "start_time": "2023-10-06T14:52:35.576492500Z"
    }
   },
   "id": "cd101360d9d70244"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "query = '( \"social simulation\" OR \"social modelling\" ) AND ( \"Large Language Models\" OR \"LLMs\" ) AND \"Computational Social Science\"'\n",
    "year= 2023\n",
    "\n",
    "\n",
    "driver.get(url + \"scholar?hl=en&q={0}&as_sdt=1&as_vis=1&as_ylo={1}&as_yhi={1}\".format(query, year))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:53:39.246704600Z",
     "start_time": "2023-10-06T14:53:38.349239900Z"
    }
   },
   "id": "d817dba1127f9b8d"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def parser(soup, page, year):\n",
    "    papers = []\n",
    "    html = soup.findAll('div', {'class': 'gs_r gs_or gs_scl'})\n",
    "    for result in html:\n",
    "        paper = {'Link': result.find('h3', {'class': \"gs_rt\"}).find('a')['href'], 'Additional link': '', 'Title': '',\n",
    "                 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '',\n",
    "                 'Year': year, 'Google page': page}\n",
    "\n",
    "        # If it does not pass at Title-Abstract-Keyword filter exclude this paper and continue\n",
    "        if not filterTitleAbsKey(paper['Link']):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            paper[\"Additional link\"] = result.find('div', {'class': \"gs_or_ggsm\"}).find('a')['href']\n",
    "        except:\n",
    "            paper[\"Additional link\"] = ''\n",
    "            print(\"NOTHING WAS FOUND\")\n",
    "\n",
    "        paper['Title'] = result.find('h3', {'class': \"gs_rt\"}).text\n",
    "        paper['Authors'] = \";\".join(\n",
    "            [\"%s:%s\" % (a.text, a['href']) for a in result.find('div', {'class': \"gs_a\"}).findAll('a')])\n",
    "\n",
    "        try:\n",
    "            paper['Abstract'] = result.find('div', {'class': \"gs_rs\"}).text\n",
    "        except:\n",
    "            print(\"NOTHING WAS FOUND\")\n",
    "            paper['Abstract'] = ''\n",
    "\n",
    "        for a in result.findAll('div', {'class': \"gs_fl\"})[-1].findAll('a'):\n",
    "            if a.text != '':\n",
    "                if a.text.startswith('Cited'):\n",
    "                    paper['Cited by'] = a.text.rstrip().split()[-1]\n",
    "                    paper['Cited list'] = url + a['href']\n",
    "                if a.text.startswith('Related'):\n",
    "                    paper['Related list'] = url + a['href']\n",
    "                if a.text.startswith('Import'):\n",
    "                    paper['Bibtex'] = requests.get(a['href']).text\n",
    "                    \n",
    "        papers.append(paper)\n",
    "        # Wait 20 seconds until the next request to google\n",
    "        time.sleep(20)\n",
    "\n",
    "    return papers, len(html)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:55:27.836002300Z",
     "start_time": "2023-10-06T14:55:27.797322500Z"
    }
   },
   "id": "123da238874b0ad7"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def filterTitleAbsKey(site):\n",
    "    try:\n",
    "        page = requests.get(site,timeout=600)\n",
    "        text = BeautifulSoup(page.text, 'lxml').get_text()\n",
    "        text = str.lower(text)\n",
    "        for terms in filter(None, picoc.values()):\n",
    "            if not terms.search(text):\n",
    "                logging.info(\"%s not passed on title-abs-key filter\", site)\n",
    "                return False\n",
    "        logging.info(\"%s passed on title-abs-key filter\", site)\n",
    "        return True\n",
    "    except requests.exceptions.Timeout:\n",
    "        logging.info(\"[TIMEOUT] Timeout on %s and not passed on title-abs-key filter. Skipping website\", site)\n",
    "    except:\n",
    "        logging.info(\"[ERROR] on %s and not passed on title-abs-key filter\", site)\n",
    "    return False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:55:29.079586700Z",
     "start_time": "2023-10-06T14:55:29.067266400Z"
    }
   },
   "id": "19f24f1eab272dfd"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "art, t = parser(BeautifulSoup(driver.page_source, 'lxml'), 1, 2023)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:55:53.563694100Z",
     "start_time": "2023-10-06T14:55:29.458456100Z"
    }
   },
   "id": "1db24912d2a11345"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "html = BeautifulSoup(driver.page_source, 'lxml').findAll('div', {'class': 'gs_r gs_or gs_scl'})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:56:52.101914600Z",
     "start_time": "2023-10-06T14:56:52.062986400Z"
    }
   },
   "id": "1510bad62586863c"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Link': 'https://osf.io/rwtzs/download', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n",
      "{'Link': 'https://arxiv.org/abs/2307.14984', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n",
      "{'Link': 'https://arxiv.org/abs/2308.11432', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n",
      "{'Link': 'https://search.proquest.com/openview/0bbaae522618c888853a4c6dbfa6a58e/1?pq-origsite=gscholar&cbl=18750&diss=y', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n",
      "{'Link': 'https://research-information.bris.ac.uk/files/364550531/FINAL_DRAFT_1.pdf', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n",
      "{'Link': 'https://arxiv.org/abs/2306.13723', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n",
      "{'Link': 'https://uwspace.uwaterloo.ca/handle/10012/19713', 'Additional link': '', 'Title': '', 'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '', 'Year': 2023, 'Google page': 1}\n"
     ]
    }
   ],
   "source": [
    "for result in html:\n",
    "    paper = {'Link': result.find('h3', {'class': \"gs_rt\"}).find('a')['href'], 'Additional link': '', 'Title': '',\n",
    "             'Authors': '', 'Abstract': '', 'Cited by': '', 'Cited list': '', 'Related list': '', 'Bibtex': '',\n",
    "             'Year': year, 'Google page': 1}\n",
    "    print(paper)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T14:57:32.929912700Z",
     "start_time": "2023-10-06T14:57:32.915827500Z"
    }
   },
   "id": "baec9a7eb170b62f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "45cbeecd53c63e8f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
